<html lang="en">
<head>
<title>Utilities - MIcroSimulation Tool (MIST)</title>
<meta http-equiv="Content-Type" content="text/html">
<meta name="description" content="MIcroSimulation Tool (MIST)">
<meta name="generator" content="makeinfo 4.7">
<link title="Top" rel="start" href="index.html#Top">
<link rel="prev" href="Reports.html#Reports" title="Reports">
<link rel="next" href="MIST-over-the-Cloud.html#MIST-over-the-Cloud" title="MIST over the Cloud">
<link href="http://www.gnu.org/software/texinfo/" rel="generator-home" title="Texinfo Homepage">
<!--
Copyright (C) 2013 Jacob Barhak

Copyright (C) 2009-2012 The Regents of the University of Michigan

This file is part of the MIcroSimulation Tool (MIST). The MIcroSimulation Tool (MIST) is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.

The MIcroSimulation Tool (MIST) is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.

Additional Clarification
------------------------

The MIcroSimulation Tool (MIST) is distributed in the hope that it will
be useful, but "as is" and WITHOUT ANY WARRANTY of any kind, including
any warranty that it will not infringe on any property rights of
another party or the IMPLIED WARRANTIES OF MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE. THE AUTHORS assume no responsibilities with
respect to the use of the MIcroSimulation Tool (MIST).

   The MIcroSimulation Tool (MIST) was derived from the Indirect
Estimation and Simulation Tool (IEST) and uses code distributed under
the IEST name. The change of the name signifies a split from the
original design that  focuses on microsimulation. For the sake of
completeness, the copyright  statement from the original tool developed
by the University of Michigan is provided below and is also mentioned
above.

ORIGINAL COPYRIGHT
------------------

Copyright (C) 2009-2012 The Regents of the University of Michigan.
Initially developed by Deanna Isaman, Jacob Barhak, Morton Brown, Wen
Ye.  Additional coding by Donghee Lee, Ray Lillywhite, Aidan Feldman.
Videos by Michael Kylman.

   This documentation and software are part of the Indirect Estimation
and Simulation Tool (IEST).  The Indirect Estimation and Simulation
Tool (IEST) is free software: you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by the
Free Software Foundation, either version 3 of the License, or (at your
option) any later version.

   The Indirect Estimation and Simulation Tool (IEST) is distributed in
the hope that it will be useful, but WITHOUT ANY WARRANTY; without even
the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
PURPOSE. See the GNU General Public License for more details.

Additional Clarification
------------------------

The Indirect Estimation and Simulation Tool (IEST) is distributed in
the hope that it will be useful, but "as is" and WITHOUT ANY WARRANTY
of any kind, including any warranty that it will not infringe on any
property rights of another party or the IMPLIED WARRANTIES OF
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. THE UNIVERSITY OF
MICHIGAN assumes no responsibilities with respect to the use of the
Indirect Estimation and Simulation Tool (IEST).-->
<meta http-equiv="Content-Style-Type" content="text/css">
<style type="text/css"><!--
  pre.display { font-family:inherit }
  pre.format  { font-family:inherit }
  pre.smalldisplay { font-family:inherit; font-size:smaller }
  pre.smallformat  { font-family:inherit; font-size:smaller }
  pre.smallexample { font-size:smaller }
  pre.smalllisp    { font-size:smaller }
  span.sc { font-variant:small-caps }
  span.roman { font-family: serif; font-weight: normal; } 
--></style>
</head>
<body>
<div class="node">
<p>
<a name="Utilities"></a>Next:&nbsp;<a rel="next" accesskey="n" href="MIST-over-the-Cloud.html#MIST-over-the-Cloud">MIST over the Cloud</a>,
Previous:&nbsp;<a rel="previous" accesskey="p" href="Reports.html#Reports">Reports</a>,
Up:&nbsp;<a rel="up" accesskey="u" href="index.html#Top">Top</a>
<hr><br>
</div>

<h2 class="chapter">14 Utilities</h2>

<p>With the details you already have, you should be able to conduct complicated simulation scenarios with various models and population sets. You are also able to produce reports to help analyze the simulations. Yet sometimes there is a need to get beyond the results of a single simulation, or there is a need to take the data outside the GUI. To support such manipulation, the system offers some python utilities that arrive with the system. The following text will explore these utilities and their usefulness by subject.

   <p>The utilities are python scripts that allow the user to perform special advanced tasks.  Here is a brief list of these scripts:

     <ul>
<li><span class="file">ConvertDataToCode.py</span> : A utility that converts <span class="file">zip</span> file generated by the system to a python script. 
<li><span class="file">MultiRunSimulation.py</span> : A utility that allows running the same simulation multiple times outside the GUI. Useful for parallel processing on multiple computers. It can be used well with <span class="file">MultiRunSimulationStatisticsAsCSV.py</span> to generate a summary statistics for repetitions. 
<li><span class="file">MultiRunCombinedReport.py</span> : A utility that allows combining results from several runs of the same model and population set into a single report. The use of this utility allows running multiple simulations in parallel and combining their results. 
<li><span class="file">MultiRunSimulationStatisticsAsCSV.py</span> : A utility that generates CSV summary reports from several runs of the same project. Combines well with <span class="file">MultiRunSimulation.py</span> that generates input files for this CSV report. The output consists of mean,STD,median,min,max of report columns with regard to different simulation results. 
<li><span class="file">MultiRunExportResultsAsCSV.py</span> : A utility that generates a CSV file containing the data from a set of result files. 
<li><span class="file">AssembleReportCSV.py</span> : A utility that assembles a CSV file from multiple CSV files generated by <span class="file">MultiRunExportResultsAsCSV.py</span>
<li><span class="file">CreatePlotsFromCSV.py</span> : A utility that constructs plots in a PDF file. The plot data is collected from a CSV file assembled by <span class="file">AssembleReportCSV.py</span>
<li><span class="file">CodeFromDocAndSpreadsheet.py</span> : A python script that converts rule text from a word document and CSV file from a spreadsheet with populations into code and a model file. This script was created to handle a specific format used with the Michigan model documentation. This file relies on a very specific format of documents and remains undocumented and should be treated as an example for programmers that want to extend the system. 
</ul>

<h3 class="section">14.1 Invoking Utility Scripts</h3>

<p>All these scripts are invoked using a similar manner. Therefore for explanation purposes we will refer to the script name, including the .py extension as: <span class="file">PythonScript.py</span>. Whenever the name <span class="file">PythonScript.py</span> is encountered, it should be replaced with the script name of interest.

   <p>The above scripts all start from the command prompt / terminal window. In Linux you can open a terminal window. In windows you can select the command prompt under the program group called accessories when you click on the windows start button on the lower left corner.  On the windows start menu, you can also select run and then type <code>cmd</code> and then press Enter to launch the command prompt.

   <p>Once you opened the terminal, you will have to change directory to your working directory by typing:

   <p><code>cd WorkingDirectoryFullPath</code>

   <p>Recall that your working directory is the directory you installed IEST and <code>WorkingDirectoryFullPath</code> means the full path name. To write the full directory name you can use the tab completion feature, or use drag and drop of a file into the command prompt window in windows and make corrections to the name that appears. Note that the directory separator on PC is the backslash character \ while on Linux it is a slash character / .

   <p>Once you are in the correct directory, you can invoke the script  <span class="file">PythonScript.py</span> by typing:

   <p><code>python PythonScript.py</code>

   <p>The invoked script will show you usage information and list its input variables. The program will then ask you to enter input through the console, each time prompting a single input. You can now follow the questions to run the script.

   <p>It is also possible to invoke the scripts with all their inputs from the command line and avoid asking the user for additional input. To do this, just add the input values after the script name:

   <p><code>python PythonScript.py InputVariable1 InputVariable2 ...</code>

   <p>Note that each script will have different requests for input variables. And that in many cases, there may be defaults for some variables making them optional. Optional variables are displayed in brackets [] in the usage information if the script is invoked with no variables.

   <p>We will now continue to discuss each utility script separately.

<h3 class="section">14.2 Conversion of Data to Code</h3>

<p>The script in focus for this topic is <span class="file">ConvertDataToCode.py</span>.

   <p>If you think about the way you work with the system, entities are created in a certain order and reference each other. The order of entity creation is important to enable certain dependencies. For example you need to define a state before you include it in a process, you need to create a model before you use it in a project, and you need to create a parameter before you use it in an expression. It is somewhat similar to building a house: you first need to build the foundations, then the main body, and only then the roof, in that order. And just like in a house, after it is built it is sometimes difficult to make a correction in foundations. This analogy of building a house may be helpful later on, for now we will get back to our system and the GUI.

   <p>Each time you create a new entity the system will add it to the database. This database can be saved and loaded by the system as a <span class="file">zip</span> file. This file is referred to many times as the data definitions file, since this file holds the entire database of entities that enables us to save and load our work. It can also contain simulation results on top of the project that created them. Think about adding entities to the system analogous to adding bricks to the house and think of the database as a snapshot of the entire house.

   <p>Think about a situation where instead of clicking your way through the system forms and entering data in a certain order, you can write down sentences that describe what you are doing in the form of instructions. Such a set of instructions can be used to create the database from scratch. This set of instructions constitutes a program that can reconstruct such a database. With analogy to the house, think about this as a plan with detailed instructions to a quick builder on how to build the house.

   <p>Now think that you already have a database <span class="file">zip</span> file and want the system to figure out what is the set of instructions that created the database <span class="file">zip</span> file. The system can do just that if you used the utility called <span class="file">ConvertDataToCode.py</span>.

   <p>This utility takes a database <span class="file">zip</span> file as input and creates Python code that reconstructs this database.  With analogy to the house, think about it as looking at a snapshot of a house and automatically deriving the plans for the house as instructions to the builder.

   <p>The main input parameter to the reconstruction program is the database <span class="file">zip</span> file we will denote as <code>DataDefintionsFileName.zip</code> and typically the script will be invoked in the following way:

   <p><code>python ConvertDataToCode.py DataDefintionsFileName.zip</code>

   <p>This will avoid asking questions from the user and just perform the conversion with default values which are recommended for most cases. By default the set of instructions to create the database will be saved as a reconstruction Python program using the file name:
<span class="file">TheGeneratedDataCode.py</span>

   <p>With regards to our analogy, think about this file as the plan containing instructions for the builder to build the house.

   <p>If you open this file, you will find instructions that create your data base in the following order:
     <ul>
<li>States - including processes
<li>Parameters
<li>Models
<li>Transitions
<li>PopulationSets
<li>Project rules ending with a project definition
</ul>

   <p>Unless you request for it specifically, simulation results will not be converted by default, otherwise these will appear at the end.

   <p>At the very end of the code, you will find a line that creates a new <span class="file">zip</span> file from the code under the default file name <span class="file">TheGeneratedDataCode_out.zip</span>. So if you run the python reconstruction program <span class="file">TheGeneratedDataCode.py</span> it will create a new database <span class="file">zip</span> file under the name <span class="file">TheGeneratedDataCode_out.zip</span> that is equivalent to the database file you converted to code.

   <p>To run the conversion from code back to data use the command:
<code>python TheGeneratedDataCode.py</code>

   <p>If there are no changes to the python reconstruction program then this allows circular path between code and data that can be followed in either direction. In other words, this allows transfer from data definitions to code and vice versa so that code and data definitions are now interchangeable. With analogy to the house, think about is as having the ability to build a house from a plan containing building instructions and the ability to take a snapshot of an existing house and convert it back into building plans. This is powerful mechanism that allows the user to make complicated changes easily.

   <p>The most useful task that can be performed through code is making changes while avoiding dependencies. For example, if the user wants to change a name of a parameter from <code>Diabetes</code> to <code>Type2Diabetes</code>, once <code>Diabetes</code> is used, the system will not allow the user to perform this change through the Graphical User Interface (GUI) since this will violate dependencies. Yet it is possible to do this using find and replace operation in the code file and then reconstructing a new data file. Note that the user should be careful to make the changes in all places and avoid name clashes and changes of other variable names with the word <code>Diabetes</code> in them. If the changes the user made in the code are reasonable, once the code is executed a new database file will be created. If changes create conflicts or are otherwise invalid, the system will not be able to reconstruct the data file. With analogy to the house, think about it as being able to take a snapshot of an existing house converting it into plans, changing the plans of the foundation and then rebuilding the entire house from the existing plan.

   <p>Note that this type of operation is intended for the advanced user and the user is responsible for making intelligent changes in the code. However, the system will make validity checks when converting the code back to data. With analogy to the house, it is up to the designer to make a proper change in the foundation in the plan, otherwise the builders will either not be able to build this house, or if the house is built, it may be faulty due to a bad change in the foundations.

   <p>There are other uses of this powerful capability code that include:
     <ol type=1 start=1>
<li>Merging different model versions by selecting wanted code lines from each version. 
<li>Conversation of code into a document or spreadsheet tables by replacing delimiting text with table separation characters and importing into a spreadsheet or a word processor application. 
<li>Finding changes between data definition files by comparing their code representation.
        </ol>

   <p>There may be other uses to this powerful capability. Yet again it is important to understand that it is not recommended for non advanced user. If not used properly, it can cause much confusion. Never the less it is a very useful tool.

   <p>As an example, it is recommended to run the following command:

   <p><code>python ConvertDataToCode.py Testing.zip</code>

   <p>This will convert the testing data definitions to code in the file <span class="file">TheGeneratedDataCode.py</span> that can be inspected by the user or executed to regenerate the data definitions.

<h3 class="section">14.3 Running Multiple Simulations</h3>

<p>The script in focus for this topic is <span class="file">MultiRunSimulation.py</span>.

   <p>Using the GUI it is possible to define and run a simulation by pressing the Run Simulation Button in the simulation screen. Each time a simulation is launched there is a need to wait for it to finish. Once done, simulation results are accessible.

   <p>However, since we typically run a Monte-Carlo simulation, we will expect different results each time we run the simulation. If we want to get a good understanding of the distribution of results, there is a need to run many repetitions of the same simulation. This is possible to do by defining a large number of repetitions for a project. However, for practical reasons it is may not be the most efficient thing to do. These reasons include: 1) Running the simulation for a very large number of population repetitions, such as 100,000 or more, may be required for some models to get stable results, yet it may take much time to wait for results. 2) Keeping simulation results in memory may not be practical as it may require larger machines and is prone to interruptions of simulation. 3) We sometimes want the population size to match the study size to allow better comparison of results. 4) Sometimes the user may wants to run the simulations outside the GUI - perhaps as a batch job.

   <p>To resolve these issues and offer further flexibility, the system provides a mechanism to run simulations outside the GUI using the <span class="file">MultiRunSimulation.py</span> script. 
When the script is invoked, it will ask for the following parameters in this order:
     <ul>
<li><code>FileName</code>: The data definitions <span class="file">zip</span> file name that holds the project to be simulated. 
<li><code>ProjectIndex</code>: The number of the project to be simulated within the data definitions <span class="file">zip</span> file. Note that project number zero means the first project on the list displayed in the GUI main screen.  However, for advanced users, it is possible to use the internal ID that can be seen if data is converted to code if it is enclosed in brackets. If this information is omitted, then the system will choose the first project by default. 
<li><code>Repetitions</code>: This is an optional integer that defines the number of times to repeat the entire simulation. For each repetition, the system will create a new output database file with simulation results for the project requested. Each new file will have the same file name as the original data definitions <span class="file">zip</span> file with an extension of a <code>_#</code> where <code>#</code> will be the number of the simulation. Each such file will be a copy of the original database with a single result set. If <code>Repetitions</code> are omitted the default is 100 repetitions. 
<li><code>StartIndex</code>: This is an optional integer that indicates the first suffix counter to be added to the file generated with the results. By default, this number will be 0, meaning that the results will be saved in a filename with the same name as the database name followed by an underscore and 0 for the first file and subsequent files generated will continue counting from this number. This number is useful if we want to add additional simulations after N simulations have already been generated by <span class="file">MultiRunSimulation.py</span> and we want to run additional simulations where filenames start their index after N simulations. This way we can save time, by running the simulations on different machines in parallel. 
<li><code>OverWriteFilesOrReconstructFromTraceback</code>: This is an optional letter, If y (default), then output files will overwrite old ones. If r then reconstruct simulation from TraceBack files in the Temp directory. The reproducibility option is useful for validation or reconstruction or simulation. 
<li><code>PopulationRepetitionsOverride</code>: This is an optional integer that defines an override to the population repetitions defined in the Project form. If the word <code>None</code> is used, then the system will not perform any override - this is the default. Otherwise the number of repetitions of each population individual is overridden. Note that <code>Repetitions</code> and <code>PopulationRepetitionsOverride</code> are related, yet define different things. In a sense these two numbers multiply the number of individuals defined in the population set if all simulations are examined. For example, if 100 Repetitions are requested as input to <span class="file">MultiRunSimulation.py</span>  and the number of <code>PopulationRepetitionsOverride</code> is 2000 for a distribution based population, then there will be 100 new files generated each with 2000 individuals simulated. If all files are counted, overall there will be 200,000 individuals simulated from which statistics can be derived. 
<li><code>ModelOverrideID</code>: This is an optional integer. The number indicates the model index to override the model defined in the project to be simulated. The first model is indexed as 0 and the model numbers are sorted according to the order they appear in the GUI in the model form when it opens. It is also possible to specify the number in brackets and then the internal index of the model will be used, this internal ID can be found if the database file is converted to code. This option is useful if the user wants to compare the results of a project with multiple model versions without redefining the data definitions <span class="file">zip</span> file. It is the responsibility of the user to make sure the model is compatible with the other definitions of the project. 
<li><code>PopulationOverrideID</code>: This is an optional integer. The number indicates the population set index to override the population set defined in the project to be simulated. The first population set is indexed as 0 and the population set numbers are sorted according to the order they appear in the GUI in the population form when it opens. It is also possible to specify the number in brackets and then the internal index of the population set will be used, this internal ID can be found if the database file is converted to code. This option is useful if the user wants to compare the results of a project with multiple population sets. It is the responsibility of the user to make sure the population set is compatible with the other definitions of the project. 
<li><code>RuleValueOverrides</code>: One or more numbers that are optional and if specified will override project initialization rule values. This is intended to allow the user to override initialization values defined in stage 0 of the simulation. The first number overrides the value provided for the affected parameter in the first rule, the second number for the second rule and so on. To use this ability the user has to define the project rules in stage 0 of the simulation to be in a known order beforehand since this order will be used to place the override values. This allows interfacing with project initialization before simulation from a batch program outside the GUI system and manipulating simulation parameters. In a sense this ability transforms the project into a function with input parameters defined by the override. 
</ul>

   <p>As an example, it is recommended to run the following command:
<code>python MultiRunSimulation.py Testing.zip 0 3</code>

   <p>This will run the first example in the file 3 times and will generate the files <span class="file">Testsing_0.zip</span>, <span class="file">Testsing_1.zip</span>, <span class="file">Testsing_2.zip</span>, each holding simulation results for the first project. You can then load these files through the GUI and inspect the results in each file.

   <p>Note that the simulation will be conducted sequentially one after the other on the same machine on the same CPU core. So using <span class="file">MultiRunSimulation.py</span> script does not save simulation time in this form. However, this script allows avoiding memory limit violations. It allows practical flexibility of conducting simulations by manipulating the simulation defaults and scaling the simulation result sizes after definitions. These capabilities can be utilized manually by the user. However, these capabilities are best utilized by the system to provide parallel computing capabilities as will be discussed later.

<h3 class="section">14.4 Generating Textual Reports from Multiple Results</h3>

<p>The script in focus for this topic is <span class="file">MultiRunCombinedReport.py</span>.

   <p>Using the GUI it was possible to generate a report for a single simulation result set. However, even within the GUI it is possible to run several simulations for the same project, each time creating a new results set while the report is per simulation results set - not per project. Moreover, if simulations for the same project were generated using <span class="file">MultiRunSimulation.py</span>, then results exist in multiple files and it is hard to compose a report for all of these together.

   <p>The <span class="file">MultiRunCombinedReport.py</span> script allows pulling together several result sets from multiple files and creating a single report combining them together. It is up to the user to make sure that the result sets are compatible.

   <p>When this script is invoked, it will ask the user a few questions as input. It is possible to answer the questions by hand, or prepare a file with the answers and run the script with this file as input as depicted in the usage.

   <p>The inputs requested are:
     <ol type=1 start=1>
<li>A list of data file names from which results will be collected, each in a separate line and a blank line to indicate the end of the list. These will be the files from which results will be pulled. 
<li>A list of simulation result ID numbers, each is a separate line with a blank line to end the list. These ID values will be searched in each file mentioned above to create the report. Typically, however, if there are multiple ID numbers defined, then there will be only one results file and vice versa. 
<li>An optional list of format options. These format options are provided as line pairs of <code>OptionName</code> and <code>OptionValue</code>. A blank line indicates the end of the format options list. Note that an easy way to obtain this list is saving the format options from the GUI results form into an <span class="file">.opt</span> file and copying the contents of this file. 
<li>The optional output report filename. If unspecified, the report name will be <span class="file">Report.txt</span>.
        </ol>

   <p>As an example that demonstrates the capabilities of this utility, we will build upon the results from the previous example created by <span class="file">MultiRunSimulation.py</span> .  In this example invoke the program in the following manner:

   <p><code>python MultiRunCombinedReport.py</code>

   <p>Then provide the following answers, where (Press Enter for Blank Line)
stands for an empty line:
<pre class="example">     Testing_0.zip
     Testing_1.zip
     Testing_2.zip
     (Press Enter for Blank Line)
     1
     (Press Enter for Blank Line)
     DetailLevel
     1
     (Press Enter for Blank Line)
     (Press Enter for Blank Line)
</pre>
   <p>These inputs can be also saved into a file that will be provided as a parameter to the script in the command line when it is invoked.

   <p>Once the script finished running, you can open the file <span class="file">Report.txt</span> and find a detailed report that will combine results from all 3 simulations in the 3 files created previously. Note that the record count is 3000 rather than 1000. Also note that the filenames are presented at the top of the report.

   <p>The <span class="file">MultiRunCombinedReport.py</span> script in combination with the previous <span class="file">MultiRunSimulation.py</span> script allows overcoming memory limitations by chopping down a large simulation to smaller chunks. This is one way to get better statistics while running a report. However, processing the report may be very time consuming, especially if there are many files since this is done sequentially. Moreover, the report will combine all individuals together into a single report so the number of individuals in the report may not match the study size. Finally, the report is textual. The system provides other tools that provide further flexibility in reporting results that are discussed next.

<h3 class="section">14.5 Generating Spreadsheet Reports from Multiple Results</h3>

<p>The script in focus for this topic is <span class="file">MultiRunSimulationStatisticsAsCSV.py</span>.

   <p>Previous reports were textual with fixed width tables, yet since most reports in the system are tabular it makes sense to create the report as a spreadsheet. A common method to represent such reports textually is the CSV format that stands for Comma Separated Values. In this format, each cell in the spreadsheet is separated from its neighbor rows using commas and a new line indicates a new row in the spreadsheet. Spreadsheet applications can open this file and the user can then manipulate it further if needed.

   <p>The script in focus is able to generate such a CSV report from a data definitions <span class="file">zip</span> file with results. Moreover, this script can do this for multiple files generated by <span class="file">MultiRunSimulation.py</span> and generate additional statistics in a summary report. Furthermore, this script allows processing this information in parallel and cutting down computation time significantly if computing power is available.

   <p>It is possible to invoke the script without input parameters in the command line and enter them manually. Yet it is usually invoked from the command line as follows:

   <p><code>python MultiRunSimulationStatisticsAsCSV.py FilePattern ResultsID OptFile OutPrefix</code>

   <p>Note that the last three command line parameters are optional and can be omitted.  Here is a description of these inputs:
     <ul>
<li><code>FilePattern</code>: The file pattern that describes the file or files to be processed. Note that this input defines the processing tasks the script will undertake.
          <ol type=1 start=1>
<li>If <code>FilePattern</code> is a single <span class="file">zip</span> file such as <span class="file">Model.zip</span>, then the system will generate a single CSV file with the same name replacing the suffix to indicate a CSV report. This is useful for running many such reports in parallel on different CPU cores. 
<li>If <code>FilePattern</code> includes wildcards that expand to multiple <span class="file">zip</span> files such as <span class="file">Model_*.zip</span> then the system will generate a CSV report for each file that matches the pattern and then an additional CSV report that summarizes these CSV files providing statistics about all files. Note that the double quotes for the file pattern are important to avoid the Linux operating system expanding this pattern before passing it to the program. Note that computations will be performed serially for each <span class="file">zip</span> file and then the report is created, this is much more time consuming than the parallel form. 
<li>If <code>FilePattern</code> includes wildcards that expand to multiple CSV files such as <code>"Model_*.CSV"</code> then the system will generate only the statistics report that summarizes these CSV files providing statistics about all files. Again, note that the double quotes are important on Linux. This form is useful in parallel computing environment if each CSV report was already computed from each <span class="file">zip</span> file in parallel as described previously.
          </ol>
<li><code>ResultsID</code>: This parameter defines the simulation result set ID to process in each file in <code>FilePattern</code>. Note that the system assumes that the results were generated by <span class="file">MultiRunSimulation.py</span> and that all results are for the same project and therefore have the same <code>ResultsID</code> in each file. Typically, the <code>ResultsID</code> will be 1 for data definitions file without previous results. The default value is <code>None</code>, meaning the first result set is selected - typically result set 1. 
<li><code>OptFile</code>: This is the report options file that can be generated and saved through the report form in the GUI. It contains report parameters of interest and calculation methods, it also contains information about stratification. Note that <code>DetailLevel</code> and other report options such as number format are ignored since these are not relevant for CSV reports. It is recommended to create such a file after running a small simulation in the GUI and compiling the report. If this file is not specified, all parameters will be used with the system trying to automatically determine the calculation method without stratification. 
<li><code>OutPrefix</code>: This defines the prefix for the output summary statistics filenames. If no prefix is defined, the system will use the common prefix of the input file names for the summary output. There will be 5 files generated as summary output, all having the same prefix and ending with the following endings: <span class="file">Mean.csv</span>, <span class="file">STD.csv</span>, <span class="file">Median.csv</span>, <span class="file">Min.csv</span>, <span class="file">Max.csv</span>. Each such ending will report the statistics for all the files that fit the <code>FilePattern</code> specified. Note that <code>OutPrefix</code> does not influence the individual CSV file generated for each <span class="file">zip</span> file, which will have the same name as the <span class="file">zip</span> file. 
</ul>

   <p>This script enables processing of reports for multiple result files and can be invoked on a machine with a single CPU, or in parallel processing environment. Here are examples that will build upon the results from the previous example created by <span class="file">MultiRunSimulation.py</span>:

   <p>Example for running simulation statistics in serial:

   <p><code>python MultiRunSimulationStatisticsAsCSV.py "Testing_*.zip"</code>

   <p>This will generate 8 CSV files: <span class="file">Testing_0.csv</span>, <span class="file">Testing_1.csv</span>, <span class="file">Testing_2.csv</span>, <span class="file">Testing_Max.csv</span>, <span class="file">Testing_Mean.csv</span>, <span class="file">Testing_Median.csv</span>, <span class="file">Testing_Min.csv</span>, <span class="file">Testing_STD.csv</span>. The first 3 files will contain a report of the results from the corresponding <span class="file">zip</span> file. The last 5 files will gather information from these 3 files and calculate a specific statistic function over these files using the functions: Max, Mean, Median, Min, STD. Note that a CSV report will look rotated compared to a textual report since columns become rows and vice versa. In the generated CSV reports, each row represents a different parameter and calculation and each column represents different time steps within a stratification cell. If there are several stratification cells, these will appear as column blocks starting with a mostly blank column defining the stratification. Note that the first few columns/rows contain headers. The statistic files also contain a new row at the end defining the number of repetitions from which the information was extracted. This is helpful to figure out how much information was available to construct the statistics. Note that in contrary to <span class="file">MultiRunCombinedReport.py</span> that combines all results and then generates a textual report on the combined population size, <span class="file">MultiRunSimulationStatisticsAsCSV.py</span> will generate multiple CSV reports for each result set using the original specified population size and provides statistics on what happens when the simulation is repeated several times.

   <p>The same example above can be repeated by running the script several times in parallel with different input parameters. To do this, the following commands should be run in parallel. This can be simulated by running the commands from multiple console/terminal windows:

   <p><code>python MultiRunSimulationStatisticsAsCSV.py Testing_0.zip</code>

   <p><code>python MultiRunSimulationStatisticsAsCSV.py Testing_1.zip</code>

   <p><code>python MultiRunSimulationStatisticsAsCSV.py Testing_2.zip</code>

   <p>Once all the above scripts have finished, run the collection script:

   <p><code>python MultiRunSimulationStatisticsAsCSV.py Testing_?.csv</code>

   <p>The first 3 commands will create a since CSV report for each <span class="file">zip</span> file, while the last command will create the 5 summary statistics CSV files from the single report CSV files. The results are similar to running the computation in the serial case, while gaining the advantage of utilizing computing power to cut down coverall computation time. This advantage is significant in High Performance Computing Environment (HPC) where this script is executed on a cluster and on the cloud, as will be shown later - see <a href="MIST-over-the-Cloud.html#MIST-over-the-Cloud">MIST over the Cloud</a>.

<h3 class="section">14.6 Assembling CSV Reports from Multiple Scenarios</h3>

<p>The script in focus for this topic is <span class="file">AssembleReportCSV.py</span>.

   <p>Typically, simulations reproduce a few different scenarios that should be compared. For example the results of a control group need to be compared to the results of an intervention group in a simulated clinical study. Once results are available, the user will want to see the results near each other on the same report using similar terminology. Alternatively, a user may want to compare simulation results to the actual results obtained from a clinical trial. Also, the user may just want to narrow down the amount of information from a single CSV report file to compare specific time frames and stratifications in a certain order from a much larger list.

   <p>The system provides some support to accommodate such comparison and visualization through the <span class="file">AssembleReportCSV.py</span> utility.

   <p>The <span class="file">AssembleReportCSV.py</span> utility assumes that <span class="file">MultiRunSimulationStatisticsAsCSV.py</span> created summary simulation reports as CSV files. And these files are to be combined to a single file that compares specific columns from those CSV files, and possibly includes reference columns from other files with a similar format.

   <p>The script is always invoked from the command line in the following format:

   <p><code>python AssembleReportCSV.py AssemblySequence OutputFileName</code>

     <ul>
<li><code>AssemblySequence</code> is an elaborate structure that allows the user to select specific columns from specific input files in a specific order. The assembly sequence will be of the form <code>[ ColumnTuple1, ColumnTuple2, ...]</code>. The user can specify this sequence within double quotes in the command line, or place it in a text file and place the filename as a command line parameter instead. Each member in the assembly sequence is a tuple enclosed in parenthesis of the form <code>(Filename, Key1, Key2, Stratification, Title)</code> where:
          <ul>
<li><code>FileName</code> is the CSV filename from which to extract the column within quotes. 
<li><code>Key1</code>: The start step of the interval of interest. This information is required and should be enclosed in quotes. 
<li><code>Key2</code>: The end step of the interval of interest. This information is required and should be enclosed in quotes. 
<li><code>Stratification</code>: This is an optional parameter that can be skipped or omitted by specifying an empty string. Otherwise, is allows specifying a stratification cell of interest by string. The string should match exactly the stratification string in the CSV report that starts with <code>'Stratification -'</code> and should be enclosed in quotes. This information allows the system to select a specific column by the stratification cell. If skipped, then the time intervals from the first stratification cell encountered will be used. 
<li><code>Title</code>: An optional parameter that can be omitted. If specified as a string in quotes, this string will be used as the column title. This allows the user to specify a title that can distinguish columns textually and give a meaningful explanation of the column and therefore recommended. 
</ul>
     <li><code>OutputFileName</code> is the name of the output CSV file where the collected columns will be placed. 
</ul>

   <p>The report generated is very similar to previous CSV reports with the difference that it can extract columns from multiple files and provides a title for each such column. So the output file contains the following information for each column: user specific title, the file name from which the column was extracted for reference, the stratification requested by the user, the project name that generated the results, the model name used in the project, the population set name used in the project, start step of interval, end step of interval, many rows with parameter statistics, repetitions count.

   <p>To make the report readable it is recommended to extract the first two header columns by including the following tuples in the beginning of the sequence:  <code>('FileName','',''), ('FileName','Start Step','End Step')</code>.  Note that this assumes that <code>&lt;Header&gt;</code> was selected as the first parameter in the report options file, which is the default.

   <p>Here is an example that builds again on the simulations we conducted using <span class="file">MultiRunSimulation.py</span> and on reports we created using <span class="file">MultiRunSimulationStatisticsAsCSV.py</span> beforehand.

   <p>Type in the following command:

   <p><code>python AssembleReportCSV.py "[('Testing_Mean.csv','',''), ('Testing_Mean.csv','Start Step','End Step'), ('Testing_0.csv','0','0','','Simulation 1 result'), ('Testing_1.csv','0','0','','Simulation 2 result'), ('Testing_2.csv','0','0','','Simulation 3 result'), ('Testing_Mean.csv','0','0','','Mean of 3 simulations') , ('Testing_STD.csv','0','0','','STD of 3 simulations'), ('Testing_0.csv','1','1','','Simulation 1 result'), ('Testing_1.csv','1','1','','Simulation 2 result'), ('Testing_2.csv','1','1','','Simulation 3 result'), ('Testing_Mean.csv','1','1','','Mean of 3 simulations') , ('Testing_STD.csv','1','1','','STD of 3 simulations'), ('Testing_0.csv','2','2','','Simulation 1 result'), ('Testing_1.csv','2','2','','Simulation 2 result'), ('Testing_2.csv','2','2','','Simulation 3 result'), ('Testing_Mean.csv','2','2','','Mean of 3 simulations') , ('Testing_STD.csv','2','2','','STD of 3 simulations'), ('Testing_0.csv','3','3','','Simulation 1 result'), ('Testing_1.csv','3','3','','Simulation 2 result'), ('Testing_2.csv','3','3','','Simulation 3 result'), ('Testing_Mean.csv','3','3','','Mean of 3 simulations') , ('Testing_STD.csv','3','3','','STD of 3 simulations')]" Testing_Out.csv</code>

   <p>This example demonstrates the use of this script to compare the results from each of the 3 simulations at all 3 years near each other. It also compares those to the Mean and STD statistics extracted for those 3 simulations.

   <p>Note that the user can specify a reference CSV file that can be used to include specific columns. Also note that the system will not check if the rows match, it just selects columns from multiple files and assembles those together. It is up to the user to make sure the columns and their definitions match between files. With good organization of the data, CSV reports can now be read by human or reused to create graphical plots as described hereafter.

<h3 class="section">14.7 Creating Graphical Plots</h3>

<p>The script in focus for this topic is <span class="file">CreatePlotsFromCSV.py</span>.

   <p>Once a CSV report is assembled, it is possible to use a spreadsheet to plot graphs using external tools. However, in many cases, there is a need to create the same plot repetitively in an automated way without manipulating the CSV file after its creation. To support such a method, the system provides the utility <span class="file">CreatePlotsFromCSV.py</span>.

   <p>This utility relies on the format that is produced by <span class="file">AssembleReportCSV.py</span> since it expects the first row to contain a title. It also expects the first two columns in the file to contain header columns with parameter and calculation method. Basically what the script does is produce a plot where the X and Y axis values are selected by the user by specifying a parameter and a calculation method. The script is sensitive to the titles provided at the first row and defines these as different series with different legends in the plots. It can also generate several plots together.

   <p>This script is invoked with the following command line:

   <p><code>python CreatePlotsFromCSV.py InputFileName OutputFileName PlotSequence</code>

     <ul>
<li><code>InputFileName</code> is the name of the file generated by <span class="file">AssembleReportCSV.py</span> and contains the data to display. 
<li><code>OutputFileName</code> is the name of the PDF document file that will contain the plots, each plot in a different page. 
<li><code>PlotSequence</code> is a file name or a string representing the graphs to be made of the form <code>[ParamList, LegendList, StyleList]</code> where <code>ParamList</code> defines what parameters are of interest in the plot, <code>LegendList</code> defines the titles of interest, <code>StyleList</code> defines the color, line type and marker to use in the plot for different legends.
          <ul>
<li><code>ParamList</code>  is of the form <code>[ParamDataX, ParamDataY1, ParamDataY2...]</code> where Each element defines a specific row in the input CSV file from which data will be extracted. The first element is considered as the series for the X axis values in the plot and therefore referred to as <code>ParamDataX</code>. Each successive <code>ParamData#</code> defines the Y axis values for a new plot and therefore named as <code>ParamDataY1</code>, <code>ParamDataY2</code>, and so on. The definition of each element <code>ParamData#</code> is the same and is normally defined as a tuple: <code>(ParamName, ParamCalcMethod, AxisTitle)</code>:
               <ul>
<li><code>ParamName</code> is the name of the parameter to display, it should be enclosed in quotes and corresponds to the names that appear in the first column in the input file. 
<li><code>ParamCalcMethod</code> is the short name for the calculation method and should correspond to the value of the second column in the CSV file. It is a required identifier to define the plot series since each parameter can be calculated several times using several calculation methods, so there is a need to define both the <code>ParamName</code> and <code>ParamCalcMethod</code> to define the correct row of values in the input CSV file. 
<li><code>AxisTitle</code> is a string enclosed in quotes that can be specified by the user to give a new name for the set of numbers in the row to appear at the axis or legend. Yet the user can specify an empty string so that the system will use the combined <code>ParamName</code> and <code>ParamCalcMethod</code> as the default axis title. 
</ul>
          <li>Recall that the first <code>ParamData#</code> parameter stands for the X axis. This X axis will be used for all the plots that will follow it and each subsequent <code>ParamData#</code> will define a new plot for a new Y axis. However, it is possible to bundle several parameters together in a single plot, or specifying a separate X axis for each plot by creating a nested <code>ParamList</code> instead of <code>ParamData#</code>. If this is done, the system will treat the nested list differently and the first <code>ParamData#</code> element will be the new X title and all subsequent <code>ParamData#</code> elements will be plotted with the new X axis all on the same plot in the same page. So nesting allows comparing different parameters, or calculation methods, or changing the X axis for this plot. Note that nesting is possible for 1 level only. 
<li>Recall that the input CSV file may contain information from several simulation scenarios, each one having a different title in the first row. The script allows selecting which scenarios the plots will be constructed from. This is done by defining <code>LegendList</code>. 
<li><code>LegendList</code> is composed of strings, enclosed in quotes, and separated by commas. The system will extract information from plots only from elements in the <code>LegendList</code>. Each such element will be displayed as a different series in the same plot with the matching legend. Note that this defines which columns from the input CSV file will be chosen. Yet the order in which those columns appear is the series will not change from the CSV file. Also note that in case of a nested <code>ParamList</code>, the name of the title will be added to the legend to separate series by legend as well as parameter and calculation method. In all cases, different series will look differently according to the sequence specified in <code>StyleList</code>. 
<li><code>StyleList</code> is a list of strings enclosed in quotes and separated by commas. These strings will determine the appearance of the line type, the color and the marker for each series in a plot. Each string is a format string where line and marker type are defined by one of the characters from the list:
<code>'-','--','-.',':','.',',','o','v','^','&lt;','&gt;','1','2','3','4','s','p','*','h','H','+','x','D','d','|','_'</code> , and color is defined by a character from the list: <code>'b','g','r','c','m','y','k','w'</code>. Combining those together will create a specific format for the line. If this list is not defined or is too short, the system will use an internal sequence of format strings. Additional information is available in
<a href="http://matplotlib.sourceforge.net/api/pyplot_api.html#matplotlib.pyplot.plot" target="_blank">this web site</a>. 
</ul>
     </ul>

   <p>The next example will demonstrate plot generation from the CSV file previously created by the <span class="file">AssembleReportCSV.py</span> example.

   <p><code>python CreatePlotsFromCSV.py Testing_Out.csv  Testing_Out.pdf "[ [('','Start Step',''), ('Alive','Sum All',''),[ ('Age','Avg All',''), ('Alive','Sum All',''), ('Dead','Sum All','')]] , ['Simulation 1 result', 'Simulation 2 result', 'Simulation 3 result', 'Mean of 3 simulations'] ,  ['r-','g-','b-','k-', 'r--','g--','b--','k--'] ]"</code>

   <p>This command will create a pdf file with two plots. The first will show the number of alive people per year for each simulation and for the average of 3 simulations. The second plot, will also show the number of deaths per year on the same plot where the X axis is age.

   <p>This plot script can be included in other scripts to build elaborate graphical reports as will be demonstrated later.

<h3 class="section">14.8 Running Simulations and Reports in Parallel on a Computer Cluster</h3>

<p>The script in focus for this topic is <code>ClusterRun.py</code>.

   <p>The utility scripts above can be used to conduct simulations, generate reports, and even create graphical plots. Those utilities run on both Linux and Windows. Those utilities can also work in High Performance Computing (HPC) environment where these can be executed on a cluster of computers. Although the system can potentially run on several HPC environments, the HPC environment of choice for the system is Sun Grid Engine (SGE). A quick start guide for SGE can be found
<a href="http://star.mit.edu/cluster/docs/0.93.3/guides/sge.html" target="_blank">here</a>.

   <p>If you have SGE installed on a computing cluster that also has all required packages installed on it, the system provides the <span class="file">ClusterRun.py</span> script that executes a complete simulation and reporting mechanism.

   <p>Note, however, that contrary to other scripts that receive input parameters when run and should not be changes, this script is a Python program that should be changed by the user to adapt to their needs. So it is assumed that the user has at least basic understanding of Python and programming. 
<a href="http://docs.python.org/tutorial/" target="_blank">This tutorial</a> may be helpful for getting acquainted with Python.

   <p><span class="file">SlurmRun.py</span> starts with a set of definitions that are intended for change by the user. After these are defined, the system will run the simulation in parallel in 3 main phases. These phases include several sub phases that will be described hereafter:

     <ul>
<li>Phase 1: Run simulation repetitions in a parallel and extract a CSV report for each run.
          <ul>
<li>Phase 1A: Run simulations using in parallel using <span class="file">MultiRunSimulation.py</span>  and create <span class="file">zip</span> files with simulation results. 
<li>Phase 1B: After the <span class="file">zip</span> file was created process simulation results using <span class="file">MultiRunSimulationStatisticsAsCSV.py</span> so that each <span class="file">zip</span> file will have a matching CSV file. 
</ul>
     <li>Phase 2: Collect all CSV files using <span class="file">MultiRunSimulationStatisticsAsCSV.py</span> to create a single CSV file for each scenario variation reporting simulation results from multiple repetitions of the same scenario variation. 
<li>Phase 3: Create final report comparing all simulations and scenario variations and notify the user.
          <ul>
<li>Phase 3A: Assemble final reports combining all simulations and possibly a reference file using <span class="file">AssembleReportCSV.py</span> to create a CSV file for summary results and a CSV file for yearly results. These reports will be assembled from all simulations and include all scenario variations in a readable manner. 
<li>Phase 3B: Analyze the results - this requires a user specific program that is not provided with the MIST distribution. 
<li>Phase 3C: Generate graphical plots using <span class="file">CreatePlotsFromCSV.py</span> that show the reports graphically. 
<li>Phase 3D: Collect all the results and send an email to the user notifying completion and summarizing results. 
</ul>
     </ul>

   <p>Using these phases, the system can run many simulations in parallel and receive many results from many scenario variations. To control the simulation, the user will change parameters in the scenario definition section at the top of the script. These parameters are:

     <ul>
<li><code>Scenario</code>: The name for the simulation job you are running. 
<li><code>FileNamePrefix</code>: The name of the <span class="file">zip</span> file that holds the data definitions of the projects to run. 
<li><code>MailFinalResultsTo</code>: The email address you want the results to be sent to. 
<li><code>Phase1Environemnt</code>, <code>Phase2Environemnt</code>, <code>Phase3Environemnt</code>: the SLURM environment parameters for the SLURM <code>sbatch</code> command you want the simulations to run with. This includes time, memory, machine allocation and many other parameters that should be determined together with the cluster administrator. 
<li><code>RunPhase1A</code>, <code>RunPhase1B</code>, <code>RunPhase2</code>, <code>RunPhase3A</code>, <code>RunPhase3B</code>, <code>RunPhase3C</code>: are Boolean parameters that allow the user to control what phases to run. These should normally be all set to <code>True</code>. However, in some cases, it is useful to have this control, especially in cases where recovery is needed. 
<li><code>ReproduceResultsFromTraceback</code>: A Boolean that controls reproducibility. If False a new simulation is created. If True the simulation is reproduces from TraceBack files in the Temp directory. 
<li><code>Repetitions</code>: The number of times to repeat each scenario variation simulation. Note that there may be several scenario variations, so the number of simulations in Phase 1 is controlled by this number and by the number of scenario variations. 
<li><code>SimulationTimeOverride</code>: This parameter can be used to override the number of simulation steps defined in the project to be run. Use <code>'None'</code> to avoid changes. 
<li><code>PopulationRepetitionsOverride</code> : This parameter can be used to override the size of population generated in the simulation by overriding the project definition of population repetitions. Use <code>'None'</code> to avoid changes. 
<li><code>OptionsSeperation1</code>, <code>OptionsSeperation2</code>, <code>OptionsSeperation3</code>, <code>OptionsSeperation4</code>, <code>OptionsSeperation5</code>, <code>OptionsSeperation6</code>: These are lists of option categories that are used to define a scenario variation. Generally elements in these lists are tuples of the form <code>(ParameterOverrideString, TitleComponent)</code>. <code>ParameterOverrideString</code> provides a sub set of parameter values to use with <span class="file">MultiRunSimulation.py</span> to override initialization values of the project in simulation stage 0. This requires for the project definition to accept these overrides. Note that these option groups are later merged to create all possible combinations of the options entered when scenario variations are determined. These options are later used during report creation to define the title from components defined in <code>TitleComponent</code>. For example, if the project accepts a single coefficient parameter that defines if biomarkers change during simulation that exists in stage 0 of simulation, we can define <code>OptionsSeperation1</code> as <code>[('0','NoBioChange'), ('1','WithBioChange')]</code>. With this example <span class="file">SlurmRun.py</span> will run both scenario options and combine them with other possible scenario options to create scenario variations. Each scenario variation created will have a title that contains either the title component <code>NoBioChange</code> or <code>WithBioChange</code>. Such titles will appear at the top of reports. Note that if there is no variation in a specific option, then the system will not include a title component for it to avoid unnecessary long title strings. As an extended example, if the project also accepts a coefficient that defines if the simulation should be run with treatment or without treatment, then the user can run both options in parallel using <span class="file">SlurmRun.py</span> if <code>OptionsSeperation2</code> is defined as <code>[('0','NoTreatment'), ('1','WithTreatment')]</code>. Note that <code>OptionsSeperation2</code> will be combined with <code>OptionsSeperation1</code> so that 2x2=4 scenario variations will be created. These scenario variations will have the following titles: <code>'NoBioChange NoTreatment'</code>, <code>'NoBioChange WithTreatment'</code>, <code>'WithBioChange NoTreatment'</code>, <code>'WithBioChange WithTreatment'</code>. These scenarios variations may be combined further with other options to create even more scenario variations. 
<li><code>StratifyBy</code>: If stratification of the results is required in the report, this string will hold the stratification table for report generation. 
<li><code>Stratifications</code>: A list of stratifications of interest in the form <code>(StatificationString, TitleComponent)</code> where <code>StatificationString</code> is the title string generated in the report that corresponds to a specific cell table specified in <code>StratifyBy</code> and starts with the words: <code>'Stratification -'</code>. <code>TitleComponent</code> is the stratification title to combine with other title components if this stratification is used in the final report. Note that this does not increase the number of simulations, yet increases the size of the report. 
<li><code>PopulationsToUse</code>, <code>ModelsToUse</code>: The population override and model override for the project. This option allows running the same simulation with multiple population sets and multiple model overrides without changing the project. The populations are defined as tuples of the form <code>(OverrideNumberAsString, TitleComponent)</code>. <code>OverrideNumberAsString</code> holds the population/model number to override as a string enclosed in quotes. If <code>OverrideNumberAsString</code> is provided in brackets, the internal code of the population set/model are used, otherwise the sort order in the GUI is used. <code>TitleComponent</code> is a string to use when the report title is assembled by the system from all options. It is up to the user to make sure the override projects/models are reasonable. 
<li><code>ProjectsToUse</code>: Allows the user to define the project number to run in different scenario variations. Again, a tuple of the form <code>(OverrideNumberAsString, TitleComponent)</code> is used. <code>OverrideNumberAsString</code> holds the project number to run from the model <span class="file">zip</span> file where the first project is indexed as 0. <code>TitleComponent</code> will determine the part of the title for the scenario variation that uses this option. If several projects are used it is up to the user to set them up so they will return results in the same format and be compatible for combining in a report. 
<li><code>Inclusions</code>, <code>Exclusions</code>: These are lists of tuples of strings that indicate what options should be included together and what options should be excluded when building the scenario variations. For example to include only scenario variations where both biomarkers and treatment or neither are simulated while disallowing other scenarios, we can define this by using <code>Inclusions = [('NoBioChange', 'NoTreatment'), ('WithBioChange', 'WithTreatment')]</code> or by defining <code>Exclusions = [('NoBioChange', 'WithTreatment'), ('WithBioChange', 'NoTreatment')]</code>. For each tuple in <code>Inclusions</code>, the system will make sure each scenario variation title that will be executed will include all the tuple components. For each tuple in <code>Exclusions</code>, the system will make sure each scenario variation title that will be executed will not include all the tuple components. By using <code>Inclusions</code> and <code>Exclusions</code> it is possible to reduce the number of scenario variations and keep only combinations of options that may interest the user, otherwise the number of scenario variations may be very large and impractical to simulate and visualize. 
<li><code>MaxDimensionsToAllowVariation</code>: This parameter allows limiting the number of scenario variations by allowing only a certain number of changes in options from the first scenario variation defined. For example if we set <code>MaxDimensionsToAllowVariation = 1</code> with the biomarker and treatment example without <code>Inclusions</code> or <code>Exclusions</code> defined, then we will get only 3 variations: <code>'NoBioChange NoTreatment'</code>, <code>'NoBioChange WithTreatment'</code>, <code>'WithBioChange NoTreatment'</code> since these change only one dimension at most from the original scenario variation. Note that <code>'WithBioChange WithTreatment'</code> will not be included since it changes both parameters from the original scenario variation. Note that the original scenario variation is determined by the first tuple defined for each option. In other words, this parameter defines the Hamming distance from the first simulation that will be considered. 
<li><code>ReportReferenceFileName</code>: This parameter allows the user to define a reference CSV file name from which the first two columns and reference values can be extracted and combined into the final report. It is useful to show known study results together with simulation results. However, the reference file should have the same format as a CSV report created by the system to allow its assembly. If an empty string is used then the system will select the first two title columns from the Mean CSV file of the first scenario. 
<li><code>ReportReferenceColumnTuple</code>: This parameter allows the user to define which columns to extract from the reference file. The tuple will be of the form <code>(ReportReferenceFileName, Key1,Key2)</code> where <code>Key1</code> and <code>Key2</code> are the column title found in rows 4,5 of the column to be extracted. If <code>ReportReferenceFileName</code> is left blank, <code>ReportReferenceColumnTuple</code> is ignored. 
<li><code>SummaryReportTimes</code>: This is a list of tuples of time step intervals to be shown in the summary report. Each tuple consists of <code>(StartTimeStep,EndTimeStep)</code> where <code>StartTimeStep</code> and <code>EndTimeStep</code> are strings enclosed in quotes that are expected to be generated in <code>SummaryIntervals</code> that is later defined. Note that <code>SummaryIntervals</code> may include other time intervals relevant to create the yearly report and other intervals while <code>SummaryReportTimes</code> selects only the intervals relevant for the final summary report, not the yearly summary report. 
<li><code>SummaryIntervals</code>: A full list of intervals for the report. It is a list of numbers and sequences to be used to process reports with. See the <a href="Reports.html#Reports">Reports</a> for further details. Note that to generate yearly summary intervals, it is recommended to include the number 1 in the sequence to generate yearly results for the yearly report and the plot. 
<li><code>ColumnFilter</code>: A list of parameters and calculation methods for reports. It is defined as a sequence of tuples <code>(ParameterName, ParameterCalculationMethod, ReplacementTitleName)</code>. See the help on <a href="Reports.html#Reports">Reports</a> for further details. It is recommended to create an <span class="file">.opt</span> file with report options and extract this filter from the file rather than construct it using the editor. 
<li><code>PlotFilter</code>: Instructions for <span class="file">CreatePlotsFromCSV.py</span> to create plots from the final yearly report. This is only the <code>ParamList</code> component from the <code>PlotSequence</code> input parameter to the <span class="file">CreatePlotsFromCSV.py</span> script. It should contain components from <code>ColumnFilter</code> where the calculation method is replaced by the short version name of the calculation method. Note that the <code>LegendList</code> component is not required since it will be calculated automatically from the titles of the scenario variations. 
<li><code>PlotStyles</code>: A sequence of strings to represent color, line style, and markers of different scenario variations. This is only the <code>StyleList</code> component from the <code>PlotSequence</code> input parameter to the <span class="file">CreatePlotsFromCSV.py</span> script. 
<li><code>ReportFilterFileName</code>: <span class="file">SlurmRun.py</span> will create a new <span class="file">.opt</span> file and save it under this name. This is created for clerical purposes to allow future manipulation of this file. 
</ul>

   <p>Running this script requires specialized environment. Yet the script as installed would run on properly installed SGE. This script is also used to run simulations on the cloud as shown in <a href="MIST-over-the-Cloud.html#MIST-over-the-Cloud">MIST over the Cloud</a>.

<h3 class="section">14.9 Extracting Results For External Processing</h3>

<p>The script in focus for this topic is <span class="file">MultiRunExportResultsAsCSV.py</span>.

   <p>The user may wish to process simulation results using different calculation techniques than those provided so far. Or the user may wish to store the calculations within a database that can be read by other systems. To provide such capabilities, the system provides a script to convert the simulation results from the internal <span class="file">zip</span> form to CSV files that can be read by many systems, including spreadsheets and database applications.

   <p>It is possible to invoke the script without input parameters in the command line and enter them manually. Yet it is usually invoked from the command line as follows:

   <p><code>python MultiRunSimulationStatisticsAsCSV.py FileNamePattern ResultsID ColumnName</code>

   <p>Where:
     <ul>
<li><code>FileNamePattern</code>: The file pattern that describes all the <span class="file">zip</span> files to be processed. It is recommended to enclose it in double quotation marks to fit both Linux and Windows formats. 
<li><code>ResultsID</code>: A mandatory parameter that defines the simulation result ID to process in each file in <code>FileNamePattern</code>. Typically, the results ID will be 1 - this is true for running a model file without previous results, never the less, the user can choose a different results set that exists in all <span class="file">zip</span> files. 
<li><code>ColumnName</code>: Optional column names that exist in the result set. The user can provide as many column names separated by spaces, these column names correspond to parameter names to be exported to the CSV file. If no column is defined, then the system will export all parameters calculated during simulation to the output file. 
</ul>
   The output file name for each file that matched the <code>FileNamePattern</code> will be the same as the file name with the <span class="file">.zip</span> ending replaced with <span class="file">Results.csv</span>. The first line in this output file will contain the parameter names to allow easier visualization and import into spreadsheet and database applications.

   <p>To demonstrate this script, here is an example that is based on the results from the previously described example of <span class="file">MultiRunSimulation.py</span>. Try running the script with the following line:

   <p><code>python MultiRunExportResultsAsCSV.py "Testing_*.zip" 1 IndividualID Repetition Time Age Alive Dead</code>

   <p>The system will create 3 files: <span class="file">Testing_0Results.csv</span> , <span class="file">Testing_1Results.csv</span>, <span class="file">Testing_2Results.csv</span>. Each file will contain 6 columns corresponding to the list provided by the user. These files are easily opened with a spreadsheet application and the results there can be manipulated further by the user to create their own reports.

   </body></html>

